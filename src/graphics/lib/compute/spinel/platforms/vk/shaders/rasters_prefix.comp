// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// FIXME(allanmac): THIS IS A PLACEHOLDER IMPLEMENTATION AS IT HAS
// SERIOUS COALESCING AND BANDWIDTH CONSUMPTION ISSUES
//

#extension GL_GOOGLE_include_directive : require
#extension GL_EXT_control_flow_attributes : require
#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require
#extension GL_KHR_shader_subgroup_vote : require

//
// PREFIX KERNEL
//

#include "spn_config.h"
#include "vk_layouts.h"

//
//
//

layout(local_size_x = SPN_DEVICE_RASTERS_PREFIX_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_RASTERS_PREFIX();

//
// LOCAL DEFINITIONS
//

// clang-format off
#define SPN_RP_SUBGROUP_SIZE  (1 << SPN_DEVICE_RASTERS_PREFIX_SUBGROUP_SIZE_LOG2)
#define SPN_RP_SUBGROUPS      (SPN_DEVICE_RASTERS_PREFIX_WORKGROUP_SIZE / SPN_RP_SUBGROUP_SIZE)
#define SPN_RP_SUBGROUP_MASK  (SPN_RP_SUBGROUP_SIZE - 1)
#define SPN_RP_TILES          (SPN_RP_SUBGROUP_SIZE / SPN_TILE_HEIGHT)
// clang-format on

//
//
//

#if (SPN_RP_TILES == 1)

#define SPN_TTSB_TILE_IDX(_sgid) 0
#define SPN_TTSB_TILE_IID(_sgid) _sgid

#else  // multiple tiles per subgroup

#define SPN_TTSB_TILE_IDX(_sgid) (_sgid >> SPN_DEVICE_TILE_HEIGHT_LOG2)
#define SPN_TTSB_TILE_IID(_sgid) (_sgid & SPN_TILE_HEIGHT_MASK)

#endif

//
// Size the smem buffer
//
// This shader only requires a subgroup of shared memory.
//

struct spn_rasters_prefix_smem
{
  int buf[SPN_RP_SUBGROUP_SIZE];
};

//
//
//

#if (SPN_RP_SUBGROUPS == 1)

shared spn_rasters_prefix_smem smem;

#define SPN_SMEM() smem

#else

shared spn_rasters_prefix_smem smem[SPN_RP_SUBGROUPS];

#define SPN_SMEM() smem[gl_SubgroupID]

#endif

//
//
//

uvec2
spn_ttrk_to_ttsk(const uvec2 ttrk)
{
  // Convert TTRK to TTSK with a span of -1.
  //
  // TTRK (64-bit COMPARE) (DEFAULT)
  //
  //  0                                                         63
  //  | TTSB_ID | NEW_X | NEW_Y |   X  |   Y  | RASTER COHORT ID |
  //  +---------+-------+-------+------+------+------------------+
  //  |    27   |   1   |   1   |  12  |  12  |        11        |
  //
  //  vvv
  //
  //  0                            63
  //  | TTSB ID |   SPAN  |  X |  Y |
  //  +---------+---------+----+----+
  //  |    27   | 13 [-1] | 12 | 12 |
  //

  // save the xy
  const uint xy = SPN_TTRK_GET_XY(ttrk);

  return uvec2(SPN_BITFIELD_INSERT(ttrk[0], -1, SPN_TTXK_LO_OFFSET_SPAN, SPN_TTXK_LO_BITS_SPAN),
               SPN_BITFIELD_INSERT(-1, xy, SPN_TTXK_HI_OFFSET_YX, SPN_TTXK_HI_BITS_YX));
}

//
//
//

int
spn_tts_get_dy(const uint tts)
{
  //
  // The tts.dy bitfield is either [-32,-1] or [0,31].
  //
  // After extracting the bitfield, the range must be adjusted:
  //
  //   if (dy >= 0) then ++dy
  //
  // The branchless equivalent subtracts the twiddle shift (~tts>>31)
  // which maps:
  //
  //   [  0,31] -> [  1,32]
  //   [-32,-1] -> [-32,-1]
  //
  // FIXME(allanmac): evaluate performance of branchless since it was
  // implemented this way in CUDA and OpenCL.
  //
  int dy = SPN_TTS_GET_DY(tts);

  if (dy >= 0)
    ++dy;

  return dy;
}

//
//
//

void
spn_ttpb_zero()
{
  SPN_SMEM().buf[gl_SubgroupInvocationID] = 0;
}

//
//
//

void
spn_ttpb_scatter(const uint tts)
{
  if (tts != SPN_TTS_INVALID)
    {
      const int dy = spn_tts_get_dy(tts);

      if (dy != 0)
        {
          const uint ty = SPN_TTSB_TILE_IDX(gl_SubgroupInvocationID) * SPN_TILE_HEIGHT +
                          SPN_TTS_GET_TY_PIXEL(tts);

          atomicAdd(SPN_SMEM().buf[ty], dy);
#if 0
          {
            const uint debug_base = atomicAdd(bp_debug_count[0], 1);

            bp_debug[debug_base] = dy;
          }
#endif
        }
    }
}

//
//
//

void
spn_ttpb_gather(const uint ttpb_id)
{
  int ttp = SPN_SMEM().buf[gl_SubgroupInvocationID];

#if (SPN_RP_TILES >= 2)

  ttp += subgroupShuffleDown(ttp, SPN_RP_SUBGROUP_SIZE / 2);

#endif
#if (SPN_RP_TILES >= 4)

  ttp += subgroupShuffleDown(ttp, SPN_RP_SUBGROUP_SIZE / 4);

#endif

#if (SPN_RP_TILES >= 8)
#error "Too many tiles!"
#endif

  //
  // store to TTPB
  //
  if (gl_SubgroupInvocationID < SPN_TILE_HEIGHT)
    {
      const uint ttpb_base = ttpb_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

      bp_blocks[ttpb_base + gl_SubgroupInvocationID] = ttp;

      //
      // DEBUG
      //
#if 0
      {
        uint debug_base = 0;

        if (gl_SubgroupInvocationID == 0)
          debug_base = atomicAdd(bp_debug_count[0], SPN_TILE_HEIGHT);

        debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

        bp_debug[debug_base] = ttp;
      }
#endif
    }
}

//
// Construct and initialize a raster object with TTSK keys, TTPK keys
// and associated TTPB blocks.
//

void
main()
{
  //
  // The raster layout is strided because it benefits raster reclamation:
  //
  //   union {
  //     u32   dwords[block_size];
  //     struct {
  //       u32 lo[block_size/2];
  //       u32 hi[block_size/2];
  //     };
  //   };
  //
  // This complicates the PREFIX and PLACE shaders.
  //
  // The raster header's .lo dwords:
  //
  //   uint32_t blocks;  // # of blocks -- head+node+skb+pkb
  //   uint32_t nodes;   // # of nodes  -- not including header
  //   uint32_t pkidx;   // absolute block pool qword of ttpk span
  //   uint32_t ttpks;   // # of ttpks
  //   uint32_t ttsks;   // # of ttsks
  //   uint32_t pknode;  // temporary: relative node of first pk
  //
  // The layout of allocated blocks is as follows:
  //
  //   ... | HEAD(1) | NODES(0+) | TTPB(0+) | ...
  //
  // Unlike previous Spinel implementations, TTPK keys immediately
  // follow TTSK keys.  The starting index of the TTPK keys in the
  // head or nodes is recorded in the raster header.
  //
  // Note that this new implementation accrues costs from redundant
  // loads from bp_ids[] as well as the ttrks_keys[] extents.  On some
  // GPUs these could be marked as uniform buffers.
  //
  // NOTE:
  //
  // There are several separable phases in this shader.  At some point
  // it might be beneficial to split them into concurrent shaders that
  // execute before the final prefix operation.
  //
  // NOTE:
  //
  // There is a subtle assumption here that the number of qwords in a
  // block is greater than or equal to the subgroup size of the target
  // device.  For example, this means that a block must be at least 64
  // qwords (128 dwords) on an AMD GCN3.  This is reasonable.
  //

  //
  // What is the cohort id for this subgroup?
  //
#if (SPN_RASTERS_PREFIX_SUBGROUPS == 1)
  SPN_SUBGROUP_UNIFORM
  const uint cid = gl_WorkGroupID.x;
#else
  SPN_SUBGROUP_UNIFORM
  const uint cid = gl_WorkGroupID.x * SPN_RP_SUBGROUPS + gl_SubgroupID;

  if (cid >= raster_span)
    return;  // empty subgroup
#endif

  //
  // Get the offset and reads for this cohort id
  //
  SPN_SUBGROUP_UNIFORM const uvec2 meta_alloc = ttrks_meta.alloc[cid];

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_RP_SUBGROUP_SIZE * 3);

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 0] = meta_alloc[0];
    bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 1] = meta_alloc[1];
    bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 2] = meta_alloc[2];
  }
#endif

  //
  // Blindly read a subgroup of ids
  //
  // clang-format off
  uint alloc_reads = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_RK_READS] + gl_SubgroupInvocationID;
  uint alloc_ids   = bp_ids[alloc_reads & bp_mask];
  // clang-format on

  //
  // The first block id is the path head block
  //
  uint head_id = subgroupBroadcast(alloc_ids, 0);

  //
  // Assumes header qwords fits in subgroup
  //
  // FIXME(allanmac): need to support (.subgroupSize = 4)
  //
  const uint header_idx = head_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + gl_SubgroupInvocationID;
  uint       header_lo  = SPN_UINT_MAX;

  if (gl_SubgroupInvocationID < SPN_RASTER_HEAD_QWORDS)
    {
      header_lo = bp_blocks[header_idx];
    }

    //
    // DEBUG
    //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], SPN_RP_SUBGROUP_SIZE * 1);

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 0] = header_lo;
  }
#endif

  //
  // How many nodes and keys?
  //
  // clang-format off
  SPN_SUBGROUP_UNIFORM uint nodes_rem = subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_NODES);
  SPN_SUBGROUP_UNIFORM uint ttsks_rem = subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_TTSKS);
  SPN_SUBGROUP_UNIFORM uint ttpks_rem = subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_TTPKS);
  // clang-format on

  //
  // Blindly finalize the final node
  //
  // clang-format off
  SPN_SUBGROUP_UNIFORM const uint ttxks_count    = SPN_RASTER_HEAD_QWORDS + ttsks_rem + ttpks_rem;
  SPN_SUBGROUP_UNIFORM const uint ttxks_count_rd = nodes_rem * (SPN_BLOCK_POOL_BLOCK_QWORDS - 1);
  SPN_SUBGROUP_UNIFORM const uint final_reads    = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_RK_READS] + nodes_rem;
  // clang-format on

  SPN_SUBGROUP_UNIFORM const uint final_id   = bp_ids[final_reads & bp_mask];
  SPN_SUBGROUP_UNIFORM const uint final_base = final_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

  //
  // FIXME(allanmac): this could be made aligned but it's probably not worth it
  //
  for (uint final_off = ttxks_count - ttxks_count_rd + gl_SubgroupInvocationID;
       final_off < SPN_BLOCK_POOL_BLOCK_QWORDS;
       final_off += SPN_RP_SUBGROUP_SIZE)
    {
      const uint final_idx = final_base + final_off;

      bp_blocks[final_idx]                               = SPN_TTXK_INVALID[0];
      bp_blocks[final_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = SPN_TTXK_INVALID[1];
    }

  //
  // If this is an empty raster then we're done!
  //
  if (ttsks_rem == 0)
    return;

  //
  // Blindly link the nodes
  //
  //   +---------------- - -
  //   | 0123 4567 89AB ...
  //   |     /    /    /
  //   | 1234 5678 9ABC ...
  //   +---------------- - -
  //
  if (nodes_rem >= 1)
    {
      while (true)
        {
          uint link_ids = subgroupShuffleDown(alloc_ids, 1);
          uint next_ids = SPN_BLOCK_ID_INVALID;

          //
          // DEBUG
          //
#if 0
          {
            uint debug_base = 0;

            if (gl_SubgroupInvocationID == 0)
              debug_base = atomicAdd(bp_debug_count[0], SPN_RP_SUBGROUP_SIZE);

            debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

            bp_debug[debug_base] = link_ids;
          }
#endif

          if (nodes_rem >= SPN_RP_SUBGROUP_SIZE)
            {
              //
              // NOTE: as noted in other shaders, Intel supports the
              // idiom below with an enhanced shuffle instruction.
              // Hopefully it comes to Vulkan (from OpenCL) via an
              // extension.
              //
              alloc_reads += SPN_RP_SUBGROUP_SIZE;

              next_ids = bp_ids[alloc_reads & bp_mask];

              const uint next_id0 = subgroupBroadcast(next_ids, 0);

              if (gl_SubgroupInvocationID == SPN_RP_SUBGROUP_SIZE - 1)
                {
                  link_ids = next_id0;
                }
            }

          if (gl_SubgroupInvocationID < nodes_rem)
            {
              const uint curr_base = alloc_ids * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

              bp_blocks[curr_base + SPN_BLOCK_POOL_BLOCK_QWORDS - 1] = link_ids;
              bp_blocks[curr_base + SPN_BLOCK_POOL_BLOCK_DWORDS - 1] = 0;
              // the upper dword must have a span of zero
            }

          // done?
          if (nodes_rem <= SPN_RP_SUBGROUP_SIZE)
            break;

          // we linked a subgroup of nodes
          nodes_rem -= SPN_RP_SUBGROUP_SIZE;

          // swap and continue
          alloc_ids = next_ids;
        }
    }

  //
  // Otherwise:
  //
  // 1. For all R ttrk keys:
  //
  //   - load ttrk keys
  //   - convert to ttsk
  //   - store ttsk to destination node
  //   - store from/to indices of ttsks for a ttpk span
  //
  // 2. Finalize the final node
  //
  // 3. subgroupBarrier()
  //
  // 4. For all P ttpk keys:
  //
  //   - scatter-accumulate ttp values
  //   - store to TTPB
  //

  //
  // FIXME(allanmac): use the head/node prefetch idiom here and make
  // reading and writing of nodes fully coalesced.
  //

  //
  // Prepare to store TTSK keys into the head node
  //
  // The subtle but safe assumption here is that the sum of header and
  // subgroup size spans less than two blocks of qwords.
  //
  uint ttrk_idx  = ttrks_meta.rk_off[cid] + gl_SubgroupInvocationID;
  uint ttsk_read = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_RK_READS];
  uint ttsk_id   = bp_ids[ttsk_read & bp_mask];
  uint ttsk_off  = SPN_RASTER_HEAD_QWORDS + gl_SubgroupInvocationID;

  // clang-format off
  SPN_SUBGROUP_UNIFORM const uint pk_idx = subgroupBroadcast(header_lo, SPN_RASTER_HEAD_LO_OFFSET_PKIDX);
  // clang-format on

  {
    uint ttpk_read = meta_alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_PK_READS];
    uint ttpk_id   = bp_ids[ttpk_read & bp_mask];
    uint ttpk_off  = pk_idx & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK;

    // running count of yx changes
    SPN_SUBGROUP_UNIFORM uint xy_count = 0;

    while (true)
      {
        // the node boundary requires special treatment
        if (ttsk_off >= SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
          {
            // reset to start of next node
            ttsk_off -= SPN_BLOCK_POOL_BLOCK_QWORDS - 1;

            // move to next node
            ttsk_read += 1;

            // load next node id -- FIXME(allanmac): make this faster
            ttsk_id = bp_ids[ttsk_read & bp_mask];
          }

        //
        // load TTRK keys and store to blocks
        //
        const uint ttsk_idx = ttsk_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + ttsk_off;
        const bool is_valid = (gl_SubgroupInvocationID < ttsks_rem);

        uvec2 ttsk;
        uint  new_xy = 0;

        if (is_valid)
          {
            // load ttrk
            const uvec2 ttrk = ttrks_keys[ttrk_idx];

            // save NEW_X and NEW_Y
            new_xy = SPN_TTRK_GET_NEW_XY(ttrk);

            // convert ttrk to ttsk
            ttsk = spn_ttrk_to_ttsk(ttrk);

            bp_blocks[ttsk_idx]                               = ttsk[0];
            bp_blocks[ttsk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = ttsk[1];
          }

        // if not zero then it's either a y or x change
        const bool is_new_xy = (new_xy != 0);

        if (subgroupAny(is_new_xy))
          {
            const uvec4 xy_ballot = subgroupBallot(is_new_xy);
            const uint  xy_exc    = subgroupBallotExclusiveBitCount(xy_ballot);
            const uint  xy_off    = xy_count + xy_exc;

            xy_count += subgroupBallotBitCount(xy_ballot);

            // tag y changes
            if (is_new_xy)
              {
                const uint smem_xy_idx = xy_off & SPN_RP_SUBGROUP_MASK;

                SPN_SMEM().buf[smem_xy_idx] = int(ttrk_idx);
              }

            const bool is_ttpk = (new_xy == SPN_TTRK_NEW_X);

#if 0
            uint debug_base = 0;

            if (cid == 2)
              {
                if (gl_SubgroupInvocationID == 0)
                  debug_base = atomicAdd(bp_debug_count[0], SPN_RP_SUBGROUP_SIZE * 5);

                debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

                bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 0] = 0xBBBBBBBB;
                bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 1] = 0xFFFFFFFF;
                bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 2] = 0xFFFFFFFF;
                bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 3] = 0xFFFFFFFF;
                bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 4] = 0xFFFFFFFF;
              }
#endif
            if (subgroupAny(is_ttpk))
              {
                // read-after-write barrier
                subgroupBarrier();

                const uvec4 ttpk_ballot = subgroupBallot(is_ttpk);
                const uint  ttpk_exc    = subgroupBallotExclusiveBitCount(ttpk_ballot);

                if (is_ttpk)
                  {
                    ttpk_off += ttpk_exc;

                    if (ttpk_off >= SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
                      {
                        // move to next node
                        ttpk_read += 1;

                        // load next node id -- FIXME(allanmac): make this faster
                        ttpk_id = bp_ids[ttpk_read & bp_mask];

                        // reset to start of next node
                        ttpk_off -= SPN_BLOCK_POOL_BLOCK_QWORDS - 1;
                      }

                    //
                    // RECORD FROM/TO TTSK INDICES
                    //
                    const uint smem_xy_idx_prev = (xy_off - 1) & SPN_RP_SUBGROUP_MASK;
                    const uint ttrk_idx_prev    = SPN_SMEM().buf[smem_xy_idx_prev];

                    const uint ttpk_idx = ttpk_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS + ttpk_off++;

                    bp_blocks[ttpk_idx]                               = ttrk_idx_prev;
                    bp_blocks[ttpk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = ttrk_idx;

                    //
                    // DEBUG
                    //
#if 0
                    if (cid == 2)
                      {
                        bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 1] = xy_exc;
                        bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 2] = ttpk_idx;
                        bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 3] = ttrk_idx_prev;
                        bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 4] = ttrk_idx;
                      }
#endif
                  }

                const uint ttpk_lane = subgroupBallotFindMSB(ttpk_ballot);

                ttpk_read = subgroupShuffle(ttpk_read, ttpk_lane);
                ttpk_id   = subgroupShuffle(ttpk_id, ttpk_lane);
                ttpk_off  = subgroupShuffle(ttpk_off, ttpk_lane);
              }
          }

        // was this the last subgroup of keys?
        if (ttsks_rem <= SPN_RP_SUBGROUP_SIZE)
          break;

        // there are still keys to process...
        ttsks_rem -= SPN_RP_SUBGROUP_SIZE;

        // bump the ttrk index
        ttrk_idx += SPN_RP_SUBGROUP_SIZE;

        // bump the node offset
        ttsk_off += SPN_RP_SUBGROUP_SIZE;
      }

    // anything to do?
    if (ttpks_rem == 0)
      return;
  }

  //
  // flush and go back to work on prefix
  //
  subgroupBarrier();

  //
  // for all quasi-TTPK keys
  //
  //   - update TTPK key with subblock id
  //   - for all TTSK keys in a TTPK key
  //     - accumulate the TTP values
  //
  {
    // ttpb subblocks start here
    SPN_SUBGROUP_UNIFORM const uint ttpb_read = final_reads + 1;

    // subblock offset
    uint ttpb_off = gl_SubgroupInvocationID;

    // directly jump to first ttpk key
    uint ttpk_base = (pk_idx & ~SPN_BLOCK_POOL_BLOCK_DWORDS_MASK);
    uint ttpk_off  = (pk_idx & SPN_BLOCK_POOL_BLOCK_DWORDS_MASK) + gl_SubgroupInvocationID;

    while (true)
      {
        //
        //           pk_idx[0] ... pk_idx[P-1]
        //              |             |
        //              |             .
        //         sk_from_to[2]      .
        //             / \            .
        //            /   \           .
        //       ttsk0 ... ttskN
        //
        const bool is_pk_valid = (gl_SubgroupInvocationID < ttpks_rem);

        // adjust pk_idx
        if (is_pk_valid)
          {
            if (ttpk_off >= SPN_BLOCK_POOL_BLOCK_QWORDS - 1)
              {
                // FIXME(allanmac): alternatively read this from the bp_ids[]
                const uint block_id = bp_blocks[ttpk_base + SPN_BLOCK_POOL_BLOCK_QWORDS - 1];

                ttpk_base = block_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

                ttpk_off -= (SPN_BLOCK_POOL_BLOCK_QWORDS - 1);
              }
          }

        //
        // load subgroup of sk_from_to[2] records
        //
        uvec2 rk_from_quit_idx;
        uint  ttpb_id;

        if (is_pk_valid)
          {
            const uint pk_idx = ttpk_base + ttpk_off;

            // get from/quit dword
            rk_from_quit_idx[0] = bp_blocks[pk_idx];
            rk_from_quit_idx[1] = bp_blocks[pk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS];

            //
            // Replace the rk_from_quit[2] with a TTPK
            //
            // get subblock id
            const uint ttpb_read_off = ttpb_off >> SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_LOG2;
            const uint ttpb_block    = ttpb_read + ttpb_read_off;
            const uint ttpb_subblock = ttpb_off & SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK_MASK;

            // load the TTPB id
            ttpb_id = bp_ids[ttpb_block & bp_mask] + ttpb_subblock;

            // get the TTSK.HI dword to obtain XY span
            const uvec2 rk_from = ttrks_keys[rk_from_quit_idx[0]];
            const uvec2 rk_quit = ttrks_keys[rk_from_quit_idx[1]];

            // get xy
            const uint rk_from_xy = SPN_TTRK_GET_XY(rk_from);
            const uint rk_quit_xy = SPN_TTRK_GET_XY(rk_quit);

            // update span
            const uint span = rk_quit_xy - rk_from_xy;

            // shift rk_from_hi
            const uint pk_from_xy = (rk_from_xy << SPN_TTXK_HI_OFFSET_YX);

            // construct TTPK key
            uvec2 ttpk = { ttpb_id, pk_from_xy + SPN_TTXK_HI_ONE_X };

            SPN_TTXK_SET_SPAN(ttpk, span);

            // write it back
            bp_blocks[pk_idx]                               = ttpk[0];
            bp_blocks[pk_idx + SPN_BLOCK_POOL_BLOCK_QWORDS] = ttpk[1];
          }

#if 0
        if (cid == 2)
          {
            uint debug_base = 0;

            if (gl_SubgroupInvocationID == 0)
              debug_base = atomicAdd(bp_debug_count[0], SPN_RP_SUBGROUP_SIZE * 3);

            debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

            bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 0] = is_pk_valid ? 0x11111111 : 0xFFFFFFFF;
            bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 1] = rk_from_quit_idx[0];
            bp_debug[debug_base + SPN_RP_SUBGROUP_SIZE * 2] = rk_from_quit_idx[1];
          }
#endif

        //
        // Accumulate altitudes for TTPB
        //
        const uint pk_valid_count = min(ttpks_rem, SPN_RP_SUBGROUP_SIZE);

        //
        // For each sk_from_to[2]:
        //   For TTSK keys in range [lo,hi)
        //     For all tiles in a subgroup
        //       - load TTSB_ID
        //       - load TTS
        //       - convert TTS to TTP
        //       - scatter TTP values
        //     End
        //   End
        //   Reduce tiles of TTP values into a TTPB
        //   Store TTPB to subblock
        //
        for (uint ii = 0; ii < pk_valid_count; ii++)
          {
            //
            // accumulate one TTPB at a time
            //
            SPN_SUBGROUP_UNIFORM const uint rk_from_idx = subgroupShuffle(rk_from_quit_idx[0], ii);
            SPN_SUBGROUP_UNIFORM const uint rk_quit_idx = subgroupShuffle(rk_from_quit_idx[1], ii);

            uint rk_idx      = rk_from_idx + SPN_TTSB_TILE_IDX(gl_SubgroupInvocationID);
            bool is_rk_valid = rk_idx < rk_quit_idx;

            uint ttrk_lo;

            if (is_rk_valid)
              {
                ttrk_lo = ttrks_keys[rk_idx][0];  // FIXME(allanmac): unfortunate
              }

            SPN_SUBGROUP_UNIFORM const uint ttrk_lo_0 = subgroupBroadcast(ttrk_lo, 0);

            // zero the accumulator?
            if (SPN_TTRK_LO_IS_NEW_Y(ttrk_lo_0))
              {
                spn_ttpb_zero();
              }

            // scatter first round
            if (is_rk_valid)
              {
                const uint ttsb_id   = SPN_TTRK_LO_GET_TTSB_ID(ttrk_lo);
                const uint ttsb_base = ttsb_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;
                const uint ttsb_idx  = ttsb_base + SPN_TTSB_TILE_IID(gl_SubgroupInvocationID);
                const uint tts       = bp_blocks[ttsb_idx];

                spn_ttpb_scatter(tts);
              }

            rk_idx += SPN_RP_TILES;

            for (; rk_idx < rk_quit_idx; rk_idx += SPN_RP_TILES)
              {
                ttrk_lo = ttrks_keys[rk_idx][0];  // FIXME(allanmac): unfortunate

                const uint ttsb_id   = SPN_TTRK_LO_GET_TTSB_ID(ttrk_lo);
                const uint ttsb_base = ttsb_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;
                const uint ttsb_idx  = ttsb_base + SPN_TTSB_TILE_IID(gl_SubgroupInvocationID);
                const uint tts       = bp_blocks[ttsb_idx];

                spn_ttpb_scatter(tts);
              }

            // read-after-write
            subgroupBarrier();

            // store accumulated altitudes to TTPB
            spn_ttpb_gather(subgroupShuffle(ttpb_id, ii));
          }

        // done?
        if (ttpks_rem <= SPN_RP_SUBGROUP_SIZE)
          break;

        ttpks_rem -= SPN_RP_SUBGROUP_SIZE;
        ttpb_off += SPN_RP_SUBGROUP_SIZE;
        ttpk_off += SPN_RP_SUBGROUP_SIZE;
      }
  }
}

//
//
//
