// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// RASTERS ALLOC
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_ballot : require

//
//
//

#include "spn_config.h"
#include "vk_layouts.h"

//
//
//

layout(local_size_x = SPN_DEVICE_RASTERS_ALLOC_WORKGROUP_SIZE) in;

//
//
//

SPN_VK_GLSL_DECL_KERNEL_RASTERS_ALLOC();

//
//
//

#define SPN_RASTERS_ALLOC_SUBGROUP_SIZE (1 << SPN_DEVICE_RASTERS_ALLOC_SUBGROUP_SIZE_LOG2)

//
// There is a fixed-size meta table per raster cohort that we use to
// peform a mostly coalesced sizing and allocation of blocks.
//
// This code is simple and fast and each lane runs in isolation except
// for a atomic operation reducing subgroup prefix sum.
//

void
main()
{
  // access to the meta extent is linear
  const bool is_active = gl_GlobalInvocationID.x < raster_span;

  // raster index
  uint raster_idx = raster_head + gl_GlobalInvocationID.x;

  if (raster_idx >= raster_size)
    {
      raster_idx -= raster_size;
    }

  //
  // active cohorts store the computed info directly to the header block
  //
  uint  raster_h;
  uvec4 header_lo;
  uint  blocks;
  uint  extra = 0;

  if (is_active)
    {
      // load raster_id as early as possible
      raster_h = raster_ids[raster_idx];

      // load meta_in
      blocks                                     = ttrks_meta.blocks[gl_GlobalInvocationID.x];
      header_lo[SPN_RASTER_HEAD_LO_OFFSET_TTSKS] = ttrks_meta.ttrks[gl_GlobalInvocationID.x];
      header_lo[SPN_RASTER_HEAD_LO_OFFSET_TTPKS] = ttrks_meta.ttpks[gl_GlobalInvocationID.x];

      // how many blocks will the ttpb vectors consume?
      extra = (header_lo[SPN_RASTER_HEAD_LO_OFFSET_TTPKS] +  //
               SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK - 1) /     //
              SPN_BLOCK_POOL_SUBBLOCKS_PER_BLOCK;            //

      // total keys
      const uint ttxks = header_lo[SPN_RASTER_HEAD_LO_OFFSET_TTSKS] +  //
                         header_lo[SPN_RASTER_HEAD_LO_OFFSET_TTPKS];   //

      //
      // how many extra blocks do we need to store all the keys in a
      // head and nodes?
      //
      // note: the "-1" is due to the final qword being used to point to
      // the next node
      //
      const uint ttxks_hn_pad = SPN_RASTER_HEAD_QWORDS + ttxks;
      const uint ttxks_hn_ru  = ttxks_hn_pad + SPN_RASTER_NODE_QWORDS - 2;
      const uint ttxks_hn     = ttxks_hn_ru / (SPN_RASTER_NODE_QWORDS - 1);

      // increment extra
      extra += ttxks_hn;

      // how many nodes trail the head?
      header_lo[SPN_RASTER_HEAD_LO_OFFSET_NODES] = ttxks_hn - 1;

      // update blocks
      blocks += extra;
    }

  //
  // allocate blocks from block pool
  //
  // first perform a prefix sum on the subgroup to reduce atomic
  // operation traffic
  //
  // note this idiom can be pushed further to operate across a workgroup
  // or operated on vectors.
  //
  const uint inc          = subgroupInclusiveAdd(extra);
  uint       bp_ids_reads = 0;

  // last lane performs the block pool alloc with an atomic increment
  if (gl_SubgroupInvocationID == SPN_RASTERS_ALLOC_SUBGROUP_SIZE - 1)
    {
      bp_ids_reads = atomicAdd(bp_atomics[SPN_BLOCK_POOL_ATOMICS_READS], inc);
    }

  // broadcast block pool base to all lanes
  SPN_SUBGROUP_UNIFORM const uint bp_ids_base =
    subgroupBroadcast(bp_ids_reads, SPN_RASTERS_ALLOC_SUBGROUP_SIZE - 1);

  //
  // store meta header
  //
  if (is_active)
    {
      // exclusive
      const uint exc = inc - extra;

      // update block pool reads base for each lane
      const uint reads = bp_ids_base + exc;

      // get block_id of each raster head
      const uint head_block_id = bp_ids[reads & bp_mask];

      // update map
      bp_host_map[raster_h] = head_block_id;

      //
      // DEBUG
      //
#if 0
      {
        const uint debug_base =
          atomicAdd(bp_debug_count[0], SPN_RASTERS_ALLOC_SUBGROUP_SIZE);

        bp_debug[debug_base + 0] = raster_h;
        bp_debug[debug_base + 1] = bp_ids_base;
        bp_debug[debug_base + 2] = exc;
        bp_debug[debug_base + 3] = reads;
        bp_debug[debug_base + 4] = bp_mask;
        bp_debug[debug_base + 5] = head_block_id;

        for (uint ii = 6; ii < SPN_RASTERS_ALLOC_SUBGROUP_SIZE; ii++)
          bp_debug[debug_base + ii] = 0xDEADBEEF;
      }
#endif

      //
      // The layout of allocated blocks is as follows:
      //
      //   ... | HEAD(1) | NODES(0+) | TTPB(0+) | ...
      //
      // Unlike previous Spinel implementations, TTPK keys immediately
      // follow TTSK keys.  The starting index of the TTPK keys in the
      // head or nodes is recorded in the raster header.
      //
      const uint pk_hn_pad  = SPN_RASTER_HEAD_QWORDS + header_lo[SPN_RASTER_HEAD_LO_OFFSET_TTSKS];
      const uint pk_hn_node = pk_hn_pad / (SPN_RASTER_NODE_QWORDS - 1);
      const uint pk_hn_base = pk_hn_node * (SPN_RASTER_NODE_QWORDS - 1);
      const uint pk_hn_off  = pk_hn_pad - pk_hn_base;

      const uint pk_hn_node_reads    = reads + pk_hn_node;
      const uint pk_hn_node_block_id = bp_ids[pk_hn_node_reads & bp_mask];
      const uint pk_hn_node_base     = pk_hn_node_block_id * SPN_BLOCK_POOL_SUBBLOCK_DWORDS;

      // absolute block pool index of starting TTPK key in raster
      header_lo[SPN_RASTER_HEAD_LO_OFFSET_PKIDX] = pk_hn_node_base + pk_hn_off;

      // save the head and pknode reads
      ttrks_meta.alloc[gl_GlobalInvocationID.x] = uvec2(reads, pk_hn_node_reads);

      //
      // Write out uvec4 and uint to header resulting in this:
      //
      //   raster head block
      //   {
      //     struct spn_raster_header.lo
      //     {
      //       uint32_t nodes;   // # of nodes  -- not including header
      //       uint32_t ttsks;   // # of ttsks
      //       uint32_t ttpks;   // # of ttpks
      //       uint32_t pkidx;   // absolute block pool qword of ttpk span
      //       uint32_t blocks;  // # of blocks -- head+node+skb+pkb
      //
      //       ... uninitialized ...
      //     }
      //     ...
      //   }
      //
      // NOTE(allanmac): We're explicitly writing a second uvec4 because
      // GLSL appears to be confused by the block pool descriptor
      // aliasing (which is technically illegal).  It's likely a
      // spirv-opt or glslangValidator error.  Once buffer references
      // are available, we can write this the correct way.
      //
      bp_blocks_uvec4[head_block_id * SPN_BLOCK_POOL_SUBBLOCK_OWORDS + 0] = header_lo;

      //
      // a smart compiler will reduce this to a dword store
      //
      uvec4 header_dword5;

      header_dword5[0] = blocks;

      bp_blocks_uvec4[head_block_id * SPN_BLOCK_POOL_SUBBLOCK_OWORDS + 1] = header_dword5;

      //
      // DEBUG
      //
#if 0
      {
        const uint debug_words =
          (8 + SPN_RASTERS_ALLOC_SUBGROUP_SIZE - 1) & ~(SPN_RASTERS_ALLOC_SUBGROUP_SIZE - 1);

        const uint debug_base = atomicAdd(bp_debug_count[0], debug_words);

        bp_debug[debug_base + 0] = raster_idx;
        bp_debug[debug_base + 1] = header_lo[0];
        bp_debug[debug_base + 2] = header_lo[1];
        bp_debug[debug_base + 3] = header_lo[2];
        bp_debug[debug_base + 4] = header_lo[3];
        bp_debug[debug_base + 5] = blocks;
        bp_debug[debug_base + 6] = alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_READS];
        bp_debug[debug_base + 7] = alloc[SPN_RASTER_COHORT_META_ALLOC_OFFSET_PKNODE];

        for (uint ii = 8; ii < debug_words; ii++)
          bp_debug[debug_base + ii] = 0xFEEDFACE;
      }
#endif
    }
}

//
//
//
