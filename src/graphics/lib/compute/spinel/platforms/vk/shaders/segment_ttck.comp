// Copyright 2019 The Fuchsia Authors. All rights reserved.
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

#version 460

//
// SEGMENT TTCK
//

#extension GL_GOOGLE_include_directive : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_ballot : require
#extension GL_KHR_shader_subgroup_shuffle : require
#extension GL_KHR_shader_subgroup_shuffle_relative : require
#extension GL_EXT_shader_explicit_arithmetic_types : require

//
// NOTE THAT THE SEGMENT TTCK KERNEL IS ENTIRELY DEPENDENT ON BOTH THE
// LAYOUT OF THE TTCK KEY AND THE SLAB GEOMETRY.
//
// IF THE TTCK KEY IS ALTERED THEN THIS KERNEL WILL NEED TO BE UPDATED
//
// NOTE THAT WE ASSUME TTCK KEYS WILL ALWAYS BE 64-BIT
//
// TTCK (32-BIT COMPARE) v1:
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          30          |    1   |    1   |   18  |  7  |  7  |
//
//
// TTCK (32-BIT COMPARE) v2:
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          30          |    1   |    1   |   15  |  9  |  8  |
//
//
// TTCK (64-BIT COMPARE) -- achieves 4K x 4K with an 8x16 tile: ( DEFAULT )
//
//  0                                                           63
//  | PAYLOAD/TTSB/TTPB ID | PREFIX | ESCAPE | LAYER |  X  |  Y  |
//  +----------------------+--------+--------+-------+-----+-----+
//  |          27          |    1   |    1   |   18  |  9  |  8  |
//

//
//
//

#include "spn_config.h"
#include "vk_layouts.h"

//
// This shader expecte uvec2 keys
//

#undef HS_ENABLE_SHUFFLE_64

#include "hs_config.h"
#include "hs_glsl_macros_config.h"

//
// specialization constants
//
//

layout(local_size_x = HS_SLAB_THREADS) in;

//
// NOTE: THIS DESCRIPTOR MUST BE COMPATIBLE WITH 'SPN_DESC_TTRKS'
//

SPN_VK_GLSL_DECL_KERNEL_SEGMENT_TTCK()

//
// clang-format off
//

#undef  HS_KV_OUT
#define HS_KV_OUT                ttcks_keys

#undef  HS_KV_OUT_LOAD
#define HS_KV_OUT_LOAD(idx)      HS_KV_OUT[idx]

#undef  HS_KV_OUT_STORE
#define HS_KV_OUT_STORE(idx, kv) HS_KV_OUT[idx] = kv

//
// clang-format on
//

#if HS_SLAB_HEIGHT > 32
#define SPN_HS_COL_FLAG_TYPE uint64_t  // FIXME(allanmac): may need to be uvec2 someday
#else
#define SPN_HS_COL_FLAG_TYPE uint
#endif

//
// MACROS
//

#define SPN_HS_IS_NOT_MAX(row_) (r##row_ != HS_KEY_VAL_MAX)

//
//
//

#define SPN_HS_IS_VALID(row_, prev_) SPN_HS_IS_NOT_MAX(row_) ? (1u << prev_) : 0

//
//
//

#define SPN_HS_HI_XOR(row_, prev_) (r##row_[1] ^ r##prev_[1])
#define SPN_HS_YX_XOR(row_, prev_) (SPN_HS_HI_XOR(row_, prev_) & SPN_TTCK_HI_MASK_YX)

//
// Note that the SPN_HS_YX_NEQ() is probably the same number of
// instructions -- eventually choose one and delete the other.
//
#if 1  // branchless
#define SPN_HS_YX_NEQ(row_, prev_) (min(1u, SPN_HS_YX_XOR(row_, prev_)) << prev_)
#else  // select
#define SPN_HS_YX_NEQ(row_, prev_) ((SPN_HS_YX_XOR(row_, prev_) != 0) ? 1u << prev_ : 0)
#endif

//
//
//

void
main()
{
  HS_SLAB_GLOBAL_IDX();

  const uint gmem_out_idx = gmem_idx;
  const uint linear_idx   = gmem_base + gmem_offset * HS_SLAB_HEIGHT;

  //
  // LOAD ALL THE ROWS
  //
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) const HS_KEY_TYPE r##row = HS_SLAB_GLOBAL_LOAD_OUT(prev);

  HS_SLAB_ROWS()

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0],  //
                             HS_KEY_DWORDS * HS_SLAB_THREADS * (HS_SLAB_HEIGHT + 4));

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + 0 * HS_SLAB_THREADS] = 0xCAFEBABE;
    bp_debug[debug_base + 1 * HS_SLAB_THREADS] = gl_WorkGroupID.x;
    bp_debug[debug_base + 2 * HS_SLAB_THREADS] = gmem_base;
    bp_debug[debug_base + 3 * HS_SLAB_THREADS] = gmem_out_idx;
    bp_debug[debug_base + 4 * HS_SLAB_THREADS] = kv_offset_in;
    bp_debug[debug_base + 5 * HS_SLAB_THREADS] = kv_offset_out;
    bp_debug[debug_base + 6 * HS_SLAB_THREADS] = kv_count;
    bp_debug[debug_base + 7 * HS_SLAB_THREADS] = 0xDEADBEEF;

    debug_base += HS_KEY_DWORDS * HS_SLAB_THREADS * 4;

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  {                                                                                                \
    const uint base = debug_base + prev * HS_SLAB_THREADS * HS_KEY_DWORDS;                         \
                                                                                                   \
    bp_debug[base]                   = r##row[0];                                                  \
    bp_debug[base + HS_SLAB_THREADS] = r##row[1];                                                  \
  }

    HS_SLAB_ROWS()
  }
#endif

  //
  // LOAD LAST REGISTER FROM COLUMN TO LEFT
  //
  // shuffle up the last key from the column to the left
  //
  // note that column 0 is undefined
  //
  HS_KEY_TYPE r0 = HS_SUBGROUP_SHUFFLE_UP(HS_REG_LAST(r), 1);

  const bool is_first_lane = (gl_SubgroupInvocationID == 0);

  if (is_first_lane)
    {
      if (gmem_base > 0)
        {
          //
          // If this is the first key in any slab but the first then it
          // broadcast loads the last key in previous slab
          //
          // Note that we only need the high dword but not all compilers
          // are cooperating right now
          //
          // FIXME(allanmac): we only need high dword (uvec2[1])
          //
          r0 = HS_KV_OUT_LOAD(gmem_base - 1);
        }
      else
        {
          //
          // If this is the first lane and first slab then record a diff
          //
          r0 = ~r1;
        }
    }

  //
  // FIND ALL VALID KEYS IN SLAB
  //
  SPN_HS_COL_FLAG_TYPE valid = 0;

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) valid |= SPN_HS_IS_VALID(row, prev);

  HS_SLAB_ROWS()

  //
  // FIND ALL DIFFERENCES IN SLAB
  //
  SPN_HS_COL_FLAG_TYPE diffs = 0;

#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev) diffs |= SPN_HS_YX_NEQ(row, prev);

  HS_SLAB_ROWS()

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], HS_SLAB_THREADS * 3);

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + 0 * HS_SLAB_THREADS] = 0xFEEDFACE;
    bp_debug[debug_base + 1 * HS_SLAB_THREADS] = valid;
    bp_debug[debug_base + 2 * HS_SLAB_THREADS] = diffs;
  }
#endif

  //
  // SUM UP THE DIFFERENCES
  //
  const SPN_HS_COL_FLAG_TYPE valid_diffs = valid & diffs;
  const uint                 count       = bitCount(valid_diffs);
  const uint                 inc         = subgroupInclusiveAdd(count);
  const uint                 exc         = inc - count;

  //
  // RESERVE SPACE IN THE INDICES ARRAY
  //
  uint next = 0;

  // FIXME(allanmac) -- need a symbolic offset
  if (gl_SubgroupInvocationID == HS_SLAB_THREADS - 1)
    {
      next = atomicAdd(offsets_count[0], inc);
    }

  // distribute base across subgroup
  next = subgroupBroadcast(next, HS_SLAB_THREADS - 1) + exc;

  //
  // DEBUG
  //
#if 0
  {
    uint debug_base = 0;

    if (gl_SubgroupInvocationID == 0)
      debug_base = atomicAdd(bp_debug_count[0], HS_SLAB_THREADS * 6);

    debug_base = subgroupBroadcast(debug_base, 0) + gl_SubgroupInvocationID;

    bp_debug[debug_base + 0 * HS_SLAB_THREADS] = 0xFEEDFACE;
    bp_debug[debug_base + 1 * HS_SLAB_THREADS] = count;
    bp_debug[debug_base + 2 * HS_SLAB_THREADS] = inc;
    bp_debug[debug_base + 3 * HS_SLAB_THREADS] = exc;
    bp_debug[debug_base + 4 * HS_SLAB_THREADS] = linear_idx;
    bp_debug[debug_base + 5 * HS_SLAB_THREADS] = next;
  }
#endif

//
// STORE THE INDICES
//
// FIXME(allanmac): Note that this is *not* going to result in
// coalesced stores but it probably doesn't matter.  If it is
// determined that this impacts performance then it's
// straightforward to accumulate these keys in local memory and
// write them out in a coalesced fashion.
//
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  if ((valid_diffs & (1 << prev)) != 0)                                                            \
    {                                                                                              \
      offsets[next++] = linear_idx + prev;                                                         \
    }

  HS_SLAB_ROWS();

#if 0
#undef HS_SLAB_ROW
#define HS_SLAB_ROW(row, prev)                                                                     \
  if ((valid_diffs & (1 << prev)) != 0)                                                            \
    {                                                                                              \
      const uint debug_base = atomicAdd(bp_debug_count[0], 1);                                     \
      bp_debug[debug_base]  = linear_idx + prev;                                                   \
    }

  HS_SLAB_ROWS();
#endif

  //
  // TRANSPOSE THE SLAB AND STORE IT
  //
  HS_TRANSPOSE_SLAB();
}

//
//
//
